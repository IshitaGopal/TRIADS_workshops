





from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from nltk.tokenize import word_tokenize
import nltk
import pandas as pd
nltk.download('punkt')

import numpy as np


reviews = pd.read_csv("https://raw.githubusercontent.com/IshitaGopal/TRIADS_workshops/main/Introduction_to_TextAnalysis/data/movie_reviews_pang02.csv")


reviews.head()


reviews.tail()


reviews.shape


reviews["class"].value_counts()


reviews["text"]


# Convert the class variable into an integer representing Pos and Neg
label_encoder = LabelEncoder()
reviews['class_int'] = label_encoder.fit_transform(reviews['class'])
reviews.head()


reviews['class_int'].value_counts()





.2*2000 # test set size


.8*2000


# Split the dataset into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(
    reviews['text'], reviews['class_int'], test_size=0.2, random_state=42
)


train_data.shape


test_data.shape






train_data.head(10)


train_labels.head(10)





# Create a CountVectorizer instance for feature extraction
vectorizer = CountVectorizer(tokenizer=word_tokenize, binary=True)



# Convert the text data into a document-term matrix
X_train_dtm = vectorizer.fit_transform(train_data)
X_test_dtm = vectorizer.transform(test_data)



X_train_dtm[0:10, 10:16].todense() # 1 represents presence of the word not counts here


# We can store the vocabulary
vocab_list = vectorizer.get_feature_names_out() # returns a list of words in alphabrtical order
vocab_dict = vectorizer.vocabulary_ # resturns a dictionary





# Create an instance of the Naive Bayes classifier
nb_classifier = BernoulliNB()

# Train (fit) the classifier using the training data
nb_classifier.fit(X_train_dtm, train_labels)



# Get (0/1) predictions for the test set
predictions_nb = nb_classifier.predict(X_test_dtm)

# Get the predicted probabilities for the test set
predicted_prob_nb = nb_classifier.predict_proba(X_test_dtm)



predictions_nb[:5]
predicted_prob_nb[:5]


list(zip(predictions_nb[:5], predicted_prob_nb[:5]))


# the sum of these 2 probabilities will equal 1
np.sum(predicted_prob_nb[:5], axis=1)





# Evaluate the classifier
accuracy = accuracy_score(test_labels, predictions_nb)
print(f'Accuracy: {accuracy:.2f}')

print("\nClassification Report:")
print(classification_report(test_labels, predictions_nb))



nb_classifier.classes_ # class order





# Identify the most important features
feature_probabilities = np.exp(nb_classifier.feature_log_prob_)






# calculate p(pos|w):
sum_feature_probabilities = np.sum(feature_probabilities, axis=0)

prob_negative_words = feature_probabilities[0,:]/sum_feature_probabilities
prob_positive_words = feature_probabilities[1,:]/sum_feature_probabilities



vocab_list[np.argsort(prob_negative_words)[-30:][::-1]]


vocab_list[np.argsort(prob_positive_words)[-30:][::-1]]





import seaborn as sns
import matplotlib.pyplot as plt

# Predicted probabilities for the positive class (stored at column index 1 in the array)
predicted_prob_neg = predicted_prob_nb[:, 0]
predicted_prob_pos = predicted_prob_nb[:, 1]

# Plot density
sns.kdeplot(predicted_prob_pos, fill=True)
plt.title("Predicted Probabilities from Naive Bayes Classifier")
plt.xlabel("")
plt.ylabel("Density")

# Add rug plot
plt.yticks([])  # Remove y-axis labels
sns.rugplot(predicted_prob_neg, color="black", height=0.1, axis="y")

plt.show()





len(test_labels)


# Find the index of the most positive/least negative probability:
## sort the predicted_prob_neg array in acsending order using np.argsort
## returns the index at the 1st position
least_negative_index = np.argsort(predicted_prob_neg)[0]

# Both of these should return the same index



least_negative_index


predicted_prob_nb[least_negative_index]


test_labels.iloc[least_negative_index]


test_data.iloc[least_negative_index]





# Find the index of the most negative/least positive probability:
least_positive_index  = np.argsort(predicted_prob_pos)[0]
least_positive_index


predicted_prob_nb[least_positive_index]


test_labels.iloc[least_positive_index]





test_data.iloc[least_positive_index]





# The index of the probability that is closest to 0.5 for the "positive" class.
most_confused_index = np.argsort(np.abs(predicted_prob_neg - 0.5))[0]


predicted_prob_nb[most_confused_index]


test_labels.iloc[most_confused_index]


test_data.iloc[most_confused_index] ##





clearly_wrong = np.argsort(predicted_prob_neg[:50])[0]


predicted_prob_nb[clearly_wrong]


test_labels.iloc[clearly_wrong]


test_data.iloc[clearly_wrong] ## Yup thats a clear mistake. overfitting?





# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report


# train_data, test_data, train_labels, test_labels
# Initialize the SVM model
svm_model = SVC(kernel='linear', C=10, probability=True)  # Use linear kernel for simplicity. You can also try other kernels like 'rbf', 'poly', etc.

# Fit the model to the training data
svm_model.fit(X_train_dtm, train_labels)


# Predicttions - predict the labels for the test data
y_pred = svm_model.predict(X_test_dtm)

# Evaluate the model
accuracy = accuracy_score(test_labels, y_pred)
report = classification_report(test_labels, y_pred)
print("Accuracy:", accuracy)
print("Summary", report)






# unique classes
print("Unique classes:", svm_model.classes_)

# weight vector or coefficient vector (tells us the weights assigned to each feature)
# larger the weight, larger the importance
print("Weight or coefficient vector:", svm_model.coef_.toarray())

# number of support vectors for each class
print("Num of support vectors for each class:", svm_model.n_support_)

# tot number of support vectors
num_support = sum(svm_model.n_support_)
print(f"Total num of support vectors:, {num_support}, This means our SVM model has identified {num_support} data points from the training set as crucial for defining the decision boundary.")

# index of the support vectors 
print("Indices of the support vectors:", svm_model.support_)

# Lagrange multiplier.y_i associated with each support vector
# This gets used when calculating the weight vector!
print("Lagrange multiplier associated with each support vector:", svm_model.dual_coef_.toarray())










# dimention of our document term matrix: 
X_train_dtm.shape
# There are 41991 features


w_direct = svm_model.coef_
w_direct.shape # we have 41991 weights corresponding to each feature/token





X_train_dtm[svm_model.support_].shape # there are 1159 support vectors and 41991 features


svm_model.dual_coef_.shape # There are 1159 alphi_i, one for every support vector





w_calc = np.dot(svm_model.dual_coef_, X_train_dtm[svm_model.support_]) # This dot product will be 1 x 41991


np.unique((w_direct == w_calc).toarray(), return_counts=True)





# Lets sort the coefficients in ascending order 
sorted_coefs = np.argsort(svm_model.coef_.toarray()[0])



# retrieves the top 30 words from the vocabulary list that are most indicative of the postive class 
# (according to their associated coefficients in the SVM model.)
print(vocab_list[sorted_coefs][-1:-31:-1])
print(svm_model.coef_.toarray()[0][sorted_coefs][-1:-31:-1])



# retrieves the top 30 words from the vocabulary list that are most indicative of the negative class 
# (according to their associated coefficients in the SVM model.)
print(vocab_list[sorted_coefs][:30])
print(svm_model.coef_.toarray()[0][sorted_coefs][:30])






import numpy as np
import matplotlib.pyplot as plt

# Assuming dfmat_train contains your training data matrix and beta_svm contains the coefficients

# Compute column sums of dfmat_train
col_sums = np.sum(X_train_dtm, axis=0).tolist()[0]
w = svm_model.coef_.toarray().tolist()[0]

plt.figure(figsize=(12, 10))

# Plot the coefficients against column sums
plt.scatter(col_sums, w, marker='o', color='black', alpha=0.2, s=20)

# Set plot properties
plt.xscale('log')
plt.xlabel('Total Appearances')
plt.ylabel('<--- Negative Reviews --- Positive Reviews --->')
plt.title('Support Vector Machine Coefficients (Linear Kernel), IMDB')

# Add text annotations
for i, txt in enumerate(vocab_list):
    plt.text(col_sums[i], w[i], txt, fontsize=70*abs(w[i]), color='black')

# Set x-axis limit
plt.xlim(1,50000)

# Show the plot
plt.show()






from sklearn.model_selection import GridSearchCV

# Define the parameter grid for grid search
# Low to High regularization cost (C) 
tune_param = {'C': [0.001, 0.1, 0.5, 1, 10, 100, 1000, 100000]}
print(tune_param)
# Define GridSearchCV with SVM classifier, parameter grid, and 5-fold cross-validation
grid_search = GridSearchCV(SVC(kernel='linear'), tune_param, cv = 5, scoring='accuracy', verbose=3)
grid_search.fit(X_train_dtm, train_labels)



# Predict using the best estimator found by grid search
y_pred = grid_search.best_estimator_.predict(X_test_dtm)

# Calculate and print the accuracy of the best estimator on the testing data
accuracy = accuracy_score(test_labels, y_pred)
print("Accuracy:", accuracy)






from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer

# Set seed (optional)
np.random.seed(1234)

# Use TfidfVectorizer for text vectorization
rf_vectorizer = CountVectorizer(min_df=50, max_df=300)

# Convert the text data into a document-term matrix
rf_X_train_dtm = rf_vectorizer.fit_transform(train_data)
rf_X_test_dtm = rf_vectorizer.transform(test_data)
rf_vocab_list = rf_vectorizer.get_feature_names_out()


#rf_model = RandomForestClassifier(n_estimators=100, random_state=42)



X_train_dtm


rf_X_train_dtm


# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_features=20)
rf_model.fit(rf_X_train_dtm, train_labels)



# Predicttions - predict the labels for the test data
rf_y_pred = rf_model.predict(rf_X_test_dtm)

# Evaluate the model
accuracy = accuracy_score(test_labels, rf_y_pred)
report = classification_report(test_labels, rf_y_pred)
print("Accuracy:", accuracy)
print("Summary", report)


# Evaluate the model
accuracy_train = rf_model.score(rf_X_train_dtm, train_labels)
print(accuracy_train)
accuracy_test = rf_model.score(rf_X_test_dtm, test_labels)
print(accuracy_test)
importance = rf_model.feature_importances_
importance


top_word_indices = np.argsort(importance)[::-1][:30]
print(rf_vocab_list[top_word_indices])
print(importance[top_word_indices])


rf_model.get_params()


len(rf_vocab_list)


mdi_importances = pd.Series(
    rf_model.feature_importances_, index=rf_vocab_list
).sort_values(ascending=True)





ax = mdi_importances[-30:].plot.barh()
ax.set_title("Random Forest Feature Importances (MDI)")
ax.figure.tight_layout()


# https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html
from sklearn.inspection import permutation_importance
result = permutation_importance(
    rf_model, rf_X_test_dtm.toarray(), test_labels, n_repeats=10, random_state=42, n_jobs=5
)




per_importance = pd.Series(result.importances_mean, index=rf_vocab_list).sort_values(ascending=False)
per_importance


import matplotlib.pyplot as plt
top_n = 30
fig, ax = plt.subplots()
per_importance[:top_n].plot.bar(yerr=result.importances_std[:top_n], ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()

















from sklearn.ensemble import VotingClassifier



# Create a voting classifier
voting_classifier = VotingClassifier(
    estimators=[('rf', rf_model), ('svm', svm_model), ('naive', )],
    voting='hard'  # Use majority voting
)


# Train the voting classifier
voting_classifier.fit(X_train_dtm, train_labels)

# Make predictions
y_pred = voting_classifier.predict(X_test_dtm)

# Evaluate accuracy
accuracy = accuracy_score(test_labels, y_pred)
print("Accuracy:", accuracy)





# Learn implementation using the easy to use Vader


%%capture
!pip install vaderSentiment


from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
# Calculate the compoud score where values below 0 = negative and above = 1
predicted_opinion_dict = test_data.apply(lambda x: 1 if analyzer.polarity_scores(x)['compound'] >= 0 else 0)


